# hydra:
#   run:
#     dir: outputs/trainer

model_name: gpt2
epochs: 4
num_workers: 8
train_batch_size: 37
data_split_percent:
  - 1
  - 1
  - 1
eval_batch_size: 5
eval_accumulation_step: 15
output_dir: results
logging_dir: logs
logging_steps: 10
max_token_len: 256
data_root: data/dstc8-schema-guided-dialogue/
project_root: /mounts/u-amo-d0/grad/adibm/projects/generative_tod/
data_prep_out_root: processed_data/simple_tod
